{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "imperial-course",
   "metadata": {},
   "source": [
    "# Project 2: What kind of wine is this?\n",
    "Michael Tobin\n",
    "LING 583"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "included-novel",
   "metadata": {},
   "source": [
    "## Deliverables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "integrated-soccer",
   "metadata": {},
   "source": [
    "#### 1. Using the data available at s3://ling583/wine-train.parquet and s3://ling583/wine-test.parquet, construct a classifier that can predict wine variety labels on the basis of review texts. Try out different methods and see what works best. Evaluate your best model using the test data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "overall-grenada",
   "metadata": {},
   "source": [
    "This can be found throughout the 5 included .ipynb files. I initially tested both Multinomial Naive Bayes and Stochastic Gradient Descent. SGD had a higher initial Macro Avgerage F-1 score so I used it for the rest of the project. This is why there is only the one MNB file and the rest are SGE; I wanted to show that an alternate classifier was tested. \n",
    "\n",
    "Initially, MNB resulted in a MA-F1 score of 0.62 which came up to 0.68 after evaluating the hyperparameters. SGD on the other hand started out at a MA-F1 score of 0.71 and the hyperparameter search was unable to improve that. The hyperparameter search found a lot of ways to make the score lower, but the highest that it was able to achieve was 0.711, which represents a sweet, sweet 1% improvement. Even so, it had a higher score than MNB, so I pushed forward. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "federal-colors",
   "metadata": {},
   "source": [
    "#### 2. Find the words that your model is using to predict labels (either by looking at the model coefficients or by using a tool like LIME). What aspects of review texts is your model most sensitive to? Is there evidence of overfitting?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "difficult-priest",
   "metadata": {},
   "source": [
    "As should probably be expected, the top words in each label were the names or some variant of the label. For example, the top words for the label 'Cabernet Sauvignon' were 'cab', 'cabernet', and 'cabs'. The bottom words almost exclusively consist of the other types of wine. That is to say, if a review mentions a Cabernet, then it is most likely not about a Syrah, for example. \n",
    "\n",
    "Once you get more towards the middle, that is the bottom couple of the top 10 and the top couple of the bottom 10, you start to see words that are not wine types but are used to describe wines. The more specific of these tend to describe the flavors of the wine and, to a more seasoned sommelier, may be used to identify a wine.\n",
    "\n",
    "\n",
    "There are a number of words that will show up regardless of type. Some examples of these include 'nose', 'body', 'acid', 'traditional', and other generalized words that talk about an attribute that any wine has, but not what that attribute means for any particular wine. These words do not appear in the top and bottom 10 words. \n",
    "\n",
    "Finally, there are some winery names and areas where the wines are from that are listed that may or may not be good indicators of the wines type. These include 'vosne', 'caymus' (winery that only produces Cabernet Suvignons), 'napa', and 'meursault'. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "described-drama",
   "metadata": {},
   "source": [
    "#### 3. For Reuters texts, we found we could greatly increase the F1 score/accuracy by excluding items that that the model was most uncertain about. How many test examples would we have to exclude to achieve better than 0.85 F1 for this task?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "african-stocks",
   "metadata": {},
   "source": [
    "I found that in order to achieve a Macro F1 score above 0.85, I had to exclude all but one review. As soon as a second review is introduced, the F1 score drops below 0.85."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baking-stewart",
   "metadata": {},
   "source": [
    "#### 4. Another way to improve accuracy is to change the labels. Use a confusion matrix to examine the patterns errors and propose a new labeling scheme. For example, if the model consistently labels “merlot” as “riesling” and vice versa, you might want to create a new label “merlot/riesling”. Is it possible to get better than 0.85 F1 using your classifier trained on a different set of labels?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vocational-marking",
   "metadata": {},
   "source": [
    "For this one I tried 3 different combinations to see if the worked. I tried the combination suggested in the assignment prompt, Merlot/Riesling, as well as Merlot/Cabernet, and Syrah/Cabernet, which represented the highest and second highest errors from the confusion matrix respecively. I found that all three produced improvements, however minimal. I made 3 different dataframes, one for each of the combinations, then ran a baseline, hyperparameter search, and comparison for each. \n",
    "\n",
    "In the end, combining Merlot and Riesling had the most drastic improvement in terms of percentage better than the baseline (3.754%), but combining Merlot with Cabernet Sauvignon resulted in the highest F1 score (0.753). None of the combinations that I tested were able to break aF1 score of 0.85."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
