{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "enhanced-motel",
   "metadata": {},
   "source": [
    "# Project 2: What kind of wine is this?\n",
    "## SGD route 1/5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "municipal-saudi",
   "metadata": {},
   "source": [
    "Construct a system to sort wine tasting notes by wine variety: Pinot Noir, Cabernet Sauvignon, Chardonnay, Syrah, Riesling, Zinfandel, Merlot, or Sauvignon Blanc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "complicated-given",
   "metadata": {},
   "source": [
    "## Deliverables\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "loaded-lewis",
   "metadata": {},
   "source": [
    "1. Using the data available at s3://ling583/wine-train.parquet and s3://ling583/wine-test.parquet, construct a classifier that can predict wine variety labels on the basis of review texts. Try out different methods and see what works best. Evaluate your best model using the test data.\n",
    "\n",
    "2. Find the words that your model is using to predict labels (either by looking at the model coefficients or by using a tool like LIME). What aspects of review texts is your model most sensitive to? Is there evidence of overfitting?\n",
    "\n",
    "3. For Reuters texts, we found we could greatly increase the F1 score/accuracy by excluding items that that the model was most uncertain about. How many test examples would we have to exclude to achieve better than 0.85 F1 for this task?\n",
    "\n",
    "4. Another way to improve accuracy is to change the labels. Use a confusion matrix to examine the patterns errors and propose a new labeling scheme. For example, if the model consistently labels “merlot” as “riesling” and vice versa, you might want to create a new label “merlot/riesling”. Is it possible to get better than 0.85 F1 using your classifier trained on a different set of labels?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "changing-gnome",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from cytoolz import *\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "tqdm.pandas()\n",
    "import multiprocessing as mp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "elementary-costa",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_parquet(\"s3://ling583/wine-train.parquet\", storage_options={\"anon\": True})\n",
    "test = pd.read_parquet(\"s3://ling583/wine-test.parquet\", storage_options={\"anon\": True})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "joint-smith",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\", \n",
    "                 exclude=[\"tagger\", \"parser\", \"ner\", \"lemmatizer\", \"attribute_ruler\"],)\n",
    "\n",
    "def tokenize(text):\n",
    "    doc = nlp.tokenizer(text)\n",
    "    return [t.norm_ for t in doc if not (t.is_space or t.is_punct or t.like_num)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "confused-treatment",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13229662a79149c59233194db86f2b56",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/130497 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d270a07f08e145d8b060500b36e37608",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/32625 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with mp.Pool() as p:\n",
    "    train[\"tokens\"] = pd.Series(p.imap(tokenize, tqdm(train[\"review_text\"]), chunksize=100))\n",
    "    test[\"tokens\"] = pd.Series(p.imap(tokenize, tqdm(test[\"review_text\"]), chunksize=100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "beginning-immigration",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pinot Noir            38471\n",
       "Cabernet Sauvignon    30234\n",
       "Chardonnay            19443\n",
       "Syrah                 13704\n",
       "Riesling               9683\n",
       "Zinfandel              8327\n",
       "Merlot                 5522\n",
       "Sauvignon Blanc        5113\n",
       "Name: wine_variant, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[\"wine_variant\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "finished-arlington",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Baseline SGD classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "warming-african",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.metrics import classification_report, f1_score\n",
    "from sklearn.pipeline import make_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "statistical-closing",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    precision    recall  f1-score   support\n",
      "\n",
      "Cabernet Sauvignon       0.72      0.78      0.75      7558\n",
      "        Chardonnay       0.82      0.85      0.83      4861\n",
      "            Merlot       0.70      0.36      0.48      1381\n",
      "        Pinot Noir       0.73      0.89      0.80      9618\n",
      "          Riesling       0.83      0.77      0.80      2421\n",
      "   Sauvignon Blanc       0.80      0.68      0.73      1278\n",
      "             Syrah       0.77      0.52      0.62      3426\n",
      "         Zinfandel       0.84      0.53      0.65      2082\n",
      "\n",
      "          accuracy                           0.76     32625\n",
      "         macro avg       0.78      0.67      0.71     32625\n",
      "      weighted avg       0.76      0.76      0.75     32625\n",
      "\n"
     ]
    }
   ],
   "source": [
    "baseline = make_pipeline(CountVectorizer(analyzer=identity), SGDClassifier())\n",
    "baseline.fit(train[\"tokens\"], train[\"wine_variant\"])\n",
    "base_predicted = baseline.predict(test[\"tokens\"])\n",
    "print(classification_report(test[\"wine_variant\"], base_predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "corresponding-battle",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "## Hyperparameter search\n",
    "\n",
    "Find an optimal set of hyperparameters for a Tfidf+SGDClassifier model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "alike-burns",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "from dask_ml.model_selection import RandomizedSearchCV\n",
    "from logger import log_search\n",
    "from scipy.stats.distributions import loguniform, randint, uniform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "normal-reality",
   "metadata": {},
   "outputs": [],
   "source": [
    "from warnings import simplefilter\n",
    "simplefilter(action=\"ignore\", category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "verified-agency",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table style=\"border: 2px solid white;\">\n",
       "<tr>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3 style=\"text-align: left;\">Client</h3>\n",
       "<ul style=\"text-align: left; list-style: none; margin: 0; padding: 0;\">\n",
       "  <li><b>Scheduler: </b>tcp://127.0.0.1:37943</li>\n",
       "  <li><b>Dashboard: </b><a href='http://127.0.0.1:8787/status' target='_blank'>http://127.0.0.1:8787/status</a></li>\n",
       "</ul>\n",
       "</td>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3 style=\"text-align: left;\">Cluster</h3>\n",
       "<ul style=\"text-align: left; list-style:none; margin: 0; padding: 0;\">\n",
       "  <li><b>Workers: </b>4</li>\n",
       "  <li><b>Cores: </b>4</li>\n",
       "  <li><b>Memory: </b>16.62 GB</li>\n",
       "</ul>\n",
       "</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<Client: 'tcp://127.0.0.1:37943' processes=4 threads=4, memory=16.62 GB>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dask.distributed import Client\n",
    "\n",
    "client = Client(\"tcp://127.0.0.1:37943\")\n",
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "compliant-insertion",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: 'Project_2_SGD' does not exist. Creating a new experiment\n"
     ]
    }
   ],
   "source": [
    "mlflow.set_experiment(\"Project_2_SGD\")\n",
    "sgd = make_pipeline(\n",
    "    CountVectorizer(analyzer=identity), TfidfTransformer(), SGDClassifier()\n",
    ")\n",
    "# Skeleton classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "adjacent-programmer",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First run: \n",
    "    # countvectorizer__min_df 1, 20\n",
    "    # countvectorizer__max_df 0.5, 0.5\n",
    "    # tfidftransformer__use_idf True, False\n",
    "    # sgdclassifier__alpha 1e-6, 1e-2\n",
    "    \n",
    "#%%time\n",
    "\n",
    "search = RandomizedSearchCV(\n",
    "    sgd,\n",
    "    {\n",
    "        \"countvectorizer__min_df\": randint(5, 10),\n",
    "        \"countvectorizer__max_df\": uniform(0.5, 0.5),\n",
    "        \"tfidftransformer__use_idf\": [True],\n",
    "        \"sgdclassifier__alpha\": loguniform(1e-6, 1e-4),\n",
    "    },\n",
    "    n_iter=50,\n",
    "    scoring=\"f1_macro\",\n",
    ")\n",
    "search.fit(train[\"tokens\"], train[\"wine_variant\"])\n",
    "log_search(search)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vocal-charlotte",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Compare optimized model to baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "baking-feeling",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    precision    recall  f1-score   support\n",
      "\n",
      "Cabernet Sauvignon       0.71      0.80      0.75      7558\n",
      "        Chardonnay       0.81      0.86      0.84      4861\n",
      "            Merlot       0.80      0.34      0.47      1381\n",
      "        Pinot Noir       0.76      0.87      0.82      9618\n",
      "          Riesling       0.81      0.78      0.80      2421\n",
      "   Sauvignon Blanc       0.82      0.67      0.74      1278\n",
      "             Syrah       0.73      0.55      0.63      3426\n",
      "         Zinfandel       0.81      0.53      0.64      2082\n",
      "\n",
      "          accuracy                           0.76     32625\n",
      "         macro avg       0.78      0.68      0.71     32625\n",
      "      weighted avg       0.76      0.76      0.75     32625\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sgd = make_pipeline(\n",
    "    CountVectorizer(analyzer=identity, min_df=5, max_df=0.7763),\n",
    "    TfidfTransformer(use_idf=True),\n",
    "    SGDClassifier(alpha=1.14e-5),\n",
    ")\n",
    "sgd.fit(train[\"tokens\"], train[\"wine_variant\"])\n",
    "predicted = sgd.predict(test[\"tokens\"])\n",
    "print(classification_report(test[\"wine_variant\"], predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "banner-criticism",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_f1 = f1_score(test[\"wine_variant\"], base_predicted, average=\"macro\")\n",
    "sgd_f1 = f1_score(test[\"wine_variant\"], predicted, average=\"macro\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "stunning-bread",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base F1 score: 0.7068352414730442\n",
      "SGD F1 score:  0.7100334194346126\n",
      "Difference:    0.0031981779615684047\n"
     ]
    }
   ],
   "source": [
    "print(f\"Base F1 score: {base_f1}\")\n",
    "print(f\"SGD F1 score:  {sgd_f1}\")\n",
    "print(f\"Difference:    {sgd_f1 - base_f1}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "extensive-communications",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.010909148758664115"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(sgd_f1 - base_f1) / (1 - base_f1)\n",
    "# Percentage error reduction; how much we imroved over the base."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "anticipated-password",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import binom_test, wilcoxon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "forty-birthday",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGD and baseline agreed 31145 times\n",
      "SGD was right, and baseline was wrong 830 times\n",
      "Baseline was right, and SGD was wrong 650 times\n"
     ]
    }
   ],
   "source": [
    "# Predicted is the SGD prediction\n",
    "# test[\"topics\"] is the right answer\n",
    "# if they are equal, the value is true, if they are not, then it is false\n",
    "diff = (predicted == test[\"wine_variant\"]).astype(int) - (base_predicted == test[\"wine_variant\"]).astype(int)\n",
    "# if both base and SGD have the same answer, thehn we get 0\n",
    "# If baseline was wrong (0) and SGD was right(1) we get 1\n",
    "# If baseline was right (1) and SGD was wrong (0) we get -1\n",
    "\n",
    "print(f\"SGD and baseline agreed {sum(diff == 0)} times\")\n",
    "print(f\"SGD was right, and baseline was wrong {sum(diff == 1)} times\")\n",
    "print(f\"Baseline was right, and SGD was wrong {sum(diff == -1)} times\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "boxed-kuwait",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.5939857540412747e-06"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# for those that were classified differently by the two classifiers, they theoretically have a 50/50 \n",
    "# chance to get into either classifier. We run the binomial test to see if the distribution of these\n",
    "# choices matches with that assumption.\n",
    "\n",
    "binom_test([sum(diff == 1), sum(diff == -1)], alternative=\"greater\")\n",
    "\n",
    "# the result, approximately 0.000000375 is much lower that the standard 0.05 alpha for the test\n",
    "# this just means that in the case of a true 50/50 chance scenario, the probability of achieving the same outcome as above is \n",
    "# incredibly small. This would indicate that the SGD classifier actually is better than the baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ready-aaron",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WilcoxonResult(statistic=614615.0, pvalue=1.4422506000125995e-06)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# similar to the binomial test above.\n",
    "# is only really applicable when you only care about the sign, plus or minus\n",
    "wilcoxon(diff, alternative=\"greater\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "matched-cigarette",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "## Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "instant-ethernet",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cloudpickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "rapid-manitoba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    precision    recall  f1-score   support\n",
      "\n",
      "Cabernet Sauvignon       0.69      0.83      0.75      7558\n",
      "        Chardonnay       0.82      0.85      0.84      4861\n",
      "            Merlot       0.82      0.33      0.48      1381\n",
      "        Pinot Noir       0.77      0.87      0.82      9618\n",
      "          Riesling       0.80      0.79      0.80      2421\n",
      "   Sauvignon Blanc       0.84      0.66      0.74      1278\n",
      "             Syrah       0.75      0.54      0.63      3426\n",
      "         Zinfandel       0.86      0.51      0.64      2082\n",
      "\n",
      "          accuracy                           0.76     32625\n",
      "         macro avg       0.80      0.67      0.71     32625\n",
      "      weighted avg       0.77      0.76      0.76     32625\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# In this version we change the preprocessor portion and add a tokenizer\n",
    "sgd = make_pipeline(\n",
    "    CountVectorizer(preprocessor=identity, tokenizer=tokenize, min_df=5, max_df=0.7763),\n",
    "    TfidfTransformer(use_idf=True),\n",
    "    SGDClassifier(alpha=1.14e-5),\n",
    ")\n",
    "sgd.fit(train[\"review_text\"], train[\"wine_variant\"])\n",
    "predicted = sgd.predict(test[\"review_text\"])\n",
    "print(classification_report(test[\"wine_variant\"], predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "fresh-safety",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The built in pickle function does not work with these complicated structures so we use cloudpickle\n",
    "cloudpickle.dump(sgd, open(\"sgd.model\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "competitive-treasurer",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
